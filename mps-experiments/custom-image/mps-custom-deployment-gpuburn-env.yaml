apiVersion: apps/v1
kind: Deployment
metadata:
  name: mps-deployment-gpuburn
  labels:
    app: mps-deployment-gpuburn
spec:
  replicas: 9
  selector:
    matchLabels:
      app: mps-deployment-gpuburn
  template:
    metadata:
      labels:
        app: mps-deployment-gpuburn
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      hostPID: true # enables Pods to talk to the MPS control daemon. It is required
      containers:
        - name: gpu-burn
          image: quay.io/mmunirab/gpu-burn:cuda12.2-built
          command: ["/bin/bash", "-c", "--"]
          args:
            - |
              ./gpu_burn 1000
          env:
            - name: CUDA_MPS_ACTIVE_THREAD_PERCENTAGE
              value: "20"
            - name: CUDA_MPS_PINNED_DEVICE_MEM_LIMIT
              value: "GPU-31cfe05c-ed13-cd17-d7aa-c63db5108c24=8000M"
          resources:
           limits:
             nvidia.com/gpu.shared: 1
